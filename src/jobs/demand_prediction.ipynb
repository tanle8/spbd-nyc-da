{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Prediction\n",
    "\n",
    "- Use date and time data from pickups to develop features for a predictive model.\n",
    "- Apply a regression model, such as linear regression, to predict the number of pickups in the next hour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Local development\n",
    "\n",
    "### Step 1. Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or retrieve a Spark session\n",
    "spark = SparkSession.builder.appName(\"Demand Prediction\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Load the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROCESSED = \"../../data/processed/train_processed.parquet\"\n",
    "\n",
    "df = spark.read.parquet(TRAIN_PROCESSED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- trip_duration: integer (nullable = true)\n",
      " |-- pickup_dayofweek: integer (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- trip_distance_km: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Feature Engineering\n",
    "\n",
    "We want to extract date and time components from the `pickup_datetime` column to use as features for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-----------+\n",
      "|year|month|day|hour|day_of_week|num_pickups|\n",
      "+----+-----+---+----+-----------+-----------+\n",
      "|2016|    3| 18|   7|          6|        393|\n",
      "|2016|    6| 30|  14|          5|        353|\n",
      "|2016|    1|  1|  14|          6|        343|\n",
      "|2016|    3| 23|  14|          4|        376|\n",
      "|2016|    3| 19|   7|          7|        157|\n",
      "|2016|    6| 29|  23|          4|        394|\n",
      "|2016|    5| 28|   3|          7|        162|\n",
      "|2016|    2| 27|   1|          7|        378|\n",
      "|2016|    4| 14|  15|          5|        459|\n",
      "|2016|    5| 10|  12|          3|        401|\n",
      "|2016|    2|  8|  11|          2|        327|\n",
      "|2016|    2| 29|  10|          2|        375|\n",
      "|2016|    1| 10|  17|          1|        375|\n",
      "|2016|    5| 13|   8|          6|        461|\n",
      "|2016|    2|  6|   3|          7|        217|\n",
      "|2016|    2| 25|  11|          5|        385|\n",
      "|2016|    5| 11|   7|          4|        373|\n",
      "|2016|    5| 14|  15|          7|        452|\n",
      "|2016|    2| 10|   0|          4|        191|\n",
      "|2016|    5|  1|  15|          1|        351|\n",
      "+----+-----+---+----+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, dayofweek\n",
    "\n",
    "# Assuming 'df' is already loaded with your data\n",
    "df = df.withColumn(\"year\", year(\"pickup_datetime\"))\n",
    "df = df.withColumn(\"month\", month(\"pickup_datetime\"))\n",
    "df = df.withColumn(\"day\", dayofmonth(\"pickup_datetime\"))\n",
    "df = df.withColumn(\"hour\", hour(\"pickup_datetime\"))\n",
    "df = df.withColumn(\"minute\", minute(\"pickup_datetime\"))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(\"pickup_datetime\"))\n",
    "\n",
    "# Group by date, hour, and day of the week to count pickups\n",
    "hourly_pickups = df.groupBy(\"year\", \"month\", \"day\", \"hour\", \"day_of_week\").count().withColumnRenamed(\"count\", \"num_pickups\")\n",
    "hourly_pickups.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation for the Model\n",
    "\n",
    "We need to split your data into training and test sets to evaluate the model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "train_data, test_data = hourly_pickups.randomSplit([0.8, 0.2], seed=1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define and Train the Regression Model\n",
    "\n",
    "Use Spark MLlib to define and train a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 15:49:10 WARN Instrumentation: [50afee1d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/05/24 15:49:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/24 15:49:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/05/24 15:49:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/05/24 15:49:11 WARN Instrumentation: [50afee1d] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "[Stage 11:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|        prediction|num_pickups|\n",
      "+------------------+-----------+\n",
      "| 201.3859906427854|        566|\n",
      "|246.72837122214892|        323|\n",
      "|503.66852783854233|        269|\n",
      "|195.22288232348671|        248|\n",
      "|210.33700918327457|        199|\n",
      "| 225.4511360430624|        143|\n",
      "|240.56526290285024|        114|\n",
      "|376.59240464094086|        361|\n",
      "| 512.6195463790315|        396|\n",
      "| 542.8478000986072|        417|\n",
      "|  196.703042398763|        138|\n",
      "|257.15954983791437|        136|\n",
      "| 272.2736766977023|        213|\n",
      "| 287.3878035574901|        281|\n",
      "| 362.9584378564293|        348|\n",
      "|408.30081843579285|        352|\n",
      "|483.87145273473203|        180|\n",
      "|145.19755350010078|        118|\n",
      "|205.65406093925216|         50|\n",
      "|326.56707581755495|        308|\n",
      "+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=[\"year\", \"month\", \"day\", \"hour\", \"day_of_week\"], outputCol=\"features\")\n",
    "\n",
    "# Initialize the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"num_pickups\")\n",
    "\n",
    "# Pipeline: Assemble vectors and then apply linear regression\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Fit the model on the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "predictions.select(\"prediction\", \"num_pickups\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the Model\n",
    "\n",
    "Evaluate the model's performance using suitable metrics, such as RMSE (Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 113.66203923029369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = RegressionEvaluator(labelCol=\"num_pickups\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data =\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Save and Deploy the Model (Optional)\n",
    "\n",
    "If the model performs satisfactorily, consider saving it for later use or deploying it for real-time predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "LOCAL_MODEL_PATH = \"../../results/models/taxi_demand_prediction_model\"\n",
    "GCP_MODEL_PATH = \"gs://spbd-nyc-taxi-bucket/results/models/taxi_demand_prediction_model\"\n",
    "\n",
    "model.save(LOCAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# from pyspark.ml import PipelineModel\n",
    "\n",
    "# # Load the model\n",
    "# model = PipelineModel.load(LOCAL_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Running on GCP\n",
    "\n",
    "After modify and refactor the code, we gonna upload and run it on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./demand_prediction.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  1.7 KiB/  1.7 KiB]                                                \n",
      "Operation completed over 1 objects/1.7 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Set bucket name\n",
    "bucket_name = \"spbd-nyc-taxi-bucket\"\n",
    "\n",
    "# Upload the Python script\n",
    "!gsutil cp ./demand_prediction.py gs://{bucket_name}/scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on operation [projects/epita-spbd-nyc-da/regions/europe-west9/operations/66fa500a-0ebf-319e-8531-e7b1655803cf].\n",
      "Waiting for cluster creation operation...                                      \n",
      "\u001b[1;33mWARNING:\u001b[0m Consider using Auto Zone rather than selecting a zone manually. See https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone\n",
      "\u001b[1;33mWARNING:\u001b[0m Failed to validate permissions required for default service account: '842263258747-compute@developer.gserviceaccount.com'. Cluster creation could still be successful if required permissions have been granted to the respective service accounts as mentioned in the document https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#dataproc_service_accounts_2. This could be due to Cloud Resource Manager API hasn't been enabled in your project '842263258747' before or it is disabled. Enable it by visiting 'https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=842263258747'.\n",
      "\u001b[1;33mWARNING:\u001b[0m The firewall rules for specified network or subnetwork would allow ingress traffic from 0.0.0.0/0, which could be a security risk.\n",
      "\u001b[1;33mWARNING:\u001b[0m The specified custom staging bucket 'dataproc-staging-europe-west9-842263258747-telg9nke' is not using uniform bucket level access IAM configuration. It is recommended to update bucket to enable the same. See https://cloud.google.com/storage/docs/uniform-bucket-level-access.\n",
      "Waiting for cluster creation operation...done.                                 \n",
      "Created [https://dataproc.googleapis.com/v1/projects/epita-spbd-nyc-da/regions/europe-west9/clusters/spbd-nyc-taxi-cluster] Cluster placed in zone [europe-west9-a].\n"
     ]
    }
   ],
   "source": [
    "cluster_name = \"spbd-nyc-taxi-cluster\"\n",
    "region = \"europe-west9\"\n",
    "machine_type=\"n2-standard-2\"\n",
    "\n",
    "!gcloud dataproc clusters create {cluster_name} \\\n",
    "    --region={region} \\\n",
    "    --zone={region}-a \\\n",
    "    --master-machine-type={machine_type} \\\n",
    "    --worker-machine-type={machine_type} \\\n",
    "    --num-workers=2 \\\n",
    "    --image-version=2.0-debian10 \\\n",
    "    --scopes=default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit our Spark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [53246a69e0a048bc8cd4ea7cc86c7e44] submitted.\n",
      "Waiting for job output...\n",
      "24/05/24 16:32:26 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "24/05/24 16:32:26 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "24/05/24 16:32:26 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/05/24 16:32:26 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "24/05/24 16:32:26 INFO org.sparkproject.jetty.util.log: Logging initialized @4056ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "24/05/24 16:32:26 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_412-b08\n",
      "24/05/24 16:32:27 INFO org.sparkproject.jetty.server.Server: Started @4208ms\n",
      "24/05/24 16:32:27 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@6dfa7a49{HTTP/1.1, (http/1.1)}{0.0.0.0:37025}\n",
      "24/05/24 16:32:27 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spbd-nyc-taxi-cluster-m/10.200.0.8:8032\n",
      "24/05/24 16:32:28 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at spbd-nyc-taxi-cluster-m/10.200.0.8:10200\n",
      "24/05/24 16:32:29 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "24/05/24 16:32:29 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/05/24 16:32:30 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1716560773834_0006\n",
      "24/05/24 16:32:31 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spbd-nyc-taxi-cluster-m/10.200.0.8:8030\n",
      "24/05/24 16:32:33 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=331; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-europe-west9-842263258747-ucxdsplb/8620a2a6-c441-452d-8a43-6d86fc80dc38/spark-job-history\n",
      "24/05/24 16:32:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "24/05/24 16:32:54 WARN org.apache.spark.ml.util.Instrumentation: [5feb71df] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/05/24 16:32:57 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/05/24 16:32:57 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "24/05/24 16:32:58 WARN com.github.fommil.netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "24/05/24 16:32:58 WARN com.github.fommil.netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "24/05/24 16:33:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://spbd-nyc-taxi-bucket/results/models/taxi_demand_prediction_model/metadata/' directory.\n",
      "24/05/24 16:33:02 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=187; previousMaxLatencyMs=0; operationCount=1; context=gs://spbd-nyc-taxi-bucket/results/models/taxi_demand_prediction_model/metadata/_temporary\n",
      "24/05/24 16:33:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://spbd-nyc-taxi-bucket/results/models/taxi_demand_prediction_model/stages/0_VectorAssembler_8196d1f604d5/metadata/' directory.\n",
      "24/05/24 16:33:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://spbd-nyc-taxi-bucket/results/models/taxi_demand_prediction_model/stages/1_LinearRegression_3db44d2e0c3a/metadata/' directory.\n",
      "24/05/24 16:33:09 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://spbd-nyc-taxi-bucket/results/models/taxi_demand_prediction_model/stages/1_LinearRegression_3db44d2e0c3a/data/' directory.\n",
      "24/05/24 16:33:09 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@6dfa7a49{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
      "24/05/24 16:33:09 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_rename. latencyMs=151; previousMaxLatencyMs=0; operationCount=1; context=rename(gs://dataproc-temp-europe-west9-842263258747-ucxdsplb/8620a2a6-c441-452d-8a43-6d86fc80dc38/spark-job-history/application_1716560773834_0006.inprogress -> gs://dataproc-temp-europe-west9-842263258747-ucxdsplb/8620a2a6-c441-452d-8a43-6d86fc80dc38/spark-job-history/application_1716560773834_0006)\n",
      "Job [53246a69e0a048bc8cd4ea7cc86c7e44] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-europe-west9-842263258747-telg9nke/google-cloud-dataproc-metainfo/8620a2a6-c441-452d-8a43-6d86fc80dc38/jobs/53246a69e0a048bc8cd4ea7cc86c7e44/\n",
      "driverOutputResourceUri: gs://dataproc-staging-europe-west9-842263258747-telg9nke/google-cloud-dataproc-metainfo/8620a2a6-c441-452d-8a43-6d86fc80dc38/jobs/53246a69e0a048bc8cd4ea7cc86c7e44/driveroutput\n",
      "jobUuid: 5942cffb-ae67-3751-9b3e-d81afe14b7dd\n",
      "placement:\n",
      "  clusterName: spbd-nyc-taxi-cluster\n",
      "  clusterUuid: 8620a2a6-c441-452d-8a43-6d86fc80dc38\n",
      "pysparkJob:\n",
      "  mainPythonFileUri: gs://spbd-nyc-taxi-bucket/scripts/demand_prediction.py\n",
      "reference:\n",
      "  jobId: 53246a69e0a048bc8cd4ea7cc86c7e44\n",
      "  projectId: epita-spbd-nyc-da\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2024-05-24T16:33:10.856631Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2024-05-24T16:32:21.878387Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2024-05-24T16:32:21.938122Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2024-05-24T16:32:22.333550Z'\n",
      "yarnApplications:\n",
      "- name: Demand Prediction\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://spbd-nyc-taxi-cluster-m:8088/proxy/application_1716560773834_0006/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark \\\n",
    "    gs://spbd-nyc-taxi-bucket/scripts/demand_prediction.py \\\n",
    "    --cluster={cluster_name} \\\n",
    "    --region={region}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the Job Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_ID                            TYPE     STATUS\n",
      "53246a69e0a048bc8cd4ea7cc86c7e44  pyspark  DONE\n",
      "04443586510448939f1aee29dcd0f55a  pyspark  ERROR\n",
      "752860f713dd42be8816269ed104e55b  pyspark  ERROR\n",
      "83c7c3998cbb421e96ebe7d478203abe  pyspark  ERROR\n",
      "3e1e1f351bab4a47b197d41c31e4f666  pyspark  ERROR\n",
      "b43ada9382d14a98b0041ca206fb2635  pyspark  ERROR\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs list --cluster=spbd-nyc-taxi-cluster --region=europe-west9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on operation [projects/epita-spbd-nyc-da/regions/europe-west9/operations/f500353b-43e2-3a3d-aad6-484ddad4771a].\n",
      "Waiting for cluster deletion operation...done.                                 \n",
      "Deleted [https://dataproc.googleapis.com/v1/projects/epita-spbd-nyc-da/regions/europe-west9/clusters/spbd-nyc-taxi-cluster].\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters delete spbd-nyc-taxi-cluster --region=europe-west9 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
