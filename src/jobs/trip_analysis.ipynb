{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Analysis\n",
    "\n",
    "Tasks:\n",
    "- Evaluate average trip durations and distances.\n",
    "- Analyze these metrics by different times of day, days of the week, and months to uncover patterns.\n",
    "- Identify the top 10 pickup and drop-off locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/21 00:04:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Trip Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "TRAIN_PROCESSED = \"../../data/processed/train_processed.parquet\"\n",
    "\n",
    "df = spark.read.parquet(TRAIN_PROCESSED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Average Trip Durations and Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------------+\n",
      "|average_trip_duration|average_trip_distance_km|\n",
      "+---------------------+------------------------+\n",
      "|    959.4922729603659|       3.440863902010865|\n",
      "+---------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate average trip duration and distance\n",
    "avg_trip_stats = df.select(\n",
    "    avg(\"trip_duration\").alias(\"average_trip_duration\"),\n",
    "    avg(\"trip_distance_km\").alias(\"average_trip_distance_km\")\n",
    ")\n",
    "avg_trip_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+------------------+------------------+\n",
      "|hour|day_of_week|month|      avg_duration|      avg_distance|\n",
      "+----+-----------+-----+------------------+------------------+\n",
      "|   0|          1|    1| 886.5953488372093|3.4501344315200257|\n",
      "|   0|          1|    2| 869.2132132132132|3.4308650655189044|\n",
      "|   0|          1|    3| 910.7371512481644|3.4837120887246047|\n",
      "|   0|          1|    4|1017.5971153846153|3.5646453594082033|\n",
      "|   0|          1|    5| 885.5396618985695|3.5978548803406696|\n",
      "|   0|          1|    6| 909.3114840062926|3.5613075183708562|\n",
      "|   1|          1|    1| 763.2528604118993|3.5783930983861048|\n",
      "|   1|          1|    2|1054.0252525252524|3.4565260078008913|\n",
      "|   1|          1|    3| 1038.832881172002| 3.585004270544865|\n",
      "|   1|          1|    4| 886.9234943027673|  3.57010856369486|\n",
      "|   1|          1|    5| 1020.668815071889| 3.706051193649675|\n",
      "|   1|          1|    6| 871.3853718500308|3.6974887099880167|\n",
      "|   2|          1|    1| 926.6111488783141|3.5897716444234544|\n",
      "|   2|          1|    2| 806.9233289646133| 3.522284805883737|\n",
      "|   2|          1|    3|  947.505928853755| 3.805873918292465|\n",
      "|   2|          1|    4| 873.1338167435729|3.5407883157386406|\n",
      "|   2|          1|    5|1087.5949214026602|3.8683675970782896|\n",
      "|   2|          1|    6| 755.6909090909091| 3.710124874808732|\n",
      "|   3|          1|    1| 790.8755760368664| 3.947902702120302|\n",
      "|   3|          1|    2|  915.942507068803|3.8306706946674955|\n",
      "+----+-----------+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek, month\n",
    "\n",
    "# Add time columns (if not already added in previous feature engineering eda notebook)\n",
    "df = df.withColumn(\"hour\", hour(\"pickup_datetime\"))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(\"pickup_datetime\"))\n",
    "df = df.withColumn(\"month\", month(\"pickup_datetime\"))\n",
    "\n",
    "# Cache the df\n",
    "df.cache()\n",
    "\n",
    "# Trigger caching with an action\n",
    "df.count()\n",
    "\n",
    "# Group by new time columns and calculate averages\n",
    "time_analysis = df.groupBy(\"hour\", \"day_of_week\", \"month\").agg(\n",
    "    avg(\"trip_duration\").alias(\"avg_duration\"),\n",
    "    avg(\"trip_distance_km\").alias(\"avg_distance\")\n",
    ").orderBy(\"day_of_week\", \"hour\", \"month\")\n",
    "\n",
    "time_analysis.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Top 10 Pickup and Drop-off Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----+\n",
      "|   pickup_latitude|  pickup_longitude|count|\n",
      "+------------------+------------------+-----+\n",
      "| 40.82100296020508|-73.95466613769531|   39|\n",
      "| 40.77378845214844|-73.87093353271484|   15|\n",
      "| 40.77376937866211|-73.87093353271484|   14|\n",
      "| 40.77376174926758| -73.8708724975586|   14|\n",
      "|40.773738861083984| -73.8708724975586|   14|\n",
      "|  40.7741584777832|-73.87303924560547|   14|\n",
      "|  40.7741813659668|-73.87300872802734|   14|\n",
      "| 40.77376174926758|-73.87091064453125|   13|\n",
      "| 40.77381134033203|-73.87095642089844|   13|\n",
      "| 40.77410888671875|-73.87303161621094|   13|\n",
      "+------------------+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----+\n",
      "|  dropoff_latitude| dropoff_longitude|count|\n",
      "+------------------+------------------+-----+\n",
      "| 40.82100296020508|-73.95466613769531|   39|\n",
      "|40.750389099121094|-73.99468231201172|   10|\n",
      "| 40.76057815551758|-74.00276947021484|   10|\n",
      "| 40.76055145263672|-74.00276947021484|    9|\n",
      "|40.750370025634766|-73.99466705322266|    8|\n",
      "| 40.75040817260742|-73.99465942382812|    8|\n",
      "| 40.75046157836914|-73.99466705322266|    8|\n",
      "|40.770591735839844|-73.86512756347656|    7|\n",
      "| 40.75014877319336|-73.99126434326172|    7|\n",
      "| 40.76839828491211|-73.86177825927734|    7|\n",
      "+------------------+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Top 10 pickup locations\n",
    "top_pickup_locations = df.groupBy(\"pickup_latitude\", \"pickup_longitude\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "top_pickup_locations.show()\n",
    "\n",
    "# Top 10 dropoff locations\n",
    "top_dropoff_locations = df.groupBy(\"dropoff_latitude\", \"dropoff_longitude\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "top_dropoff_locations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, vendor_id: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: int, pickup_longitude: double, pickup_latitude: double, dropoff_longitude: double, dropoff_latitude: double, store_and_fwd_flag: string, trip_duration: int, pickup_dayofweek: int, pickup_hour: int, pickup_month: int, pickup_year: int, trip_distance_km: double, hour: int, day_of_week: int, month: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpersist the df to free up resources\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the current session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Running on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./trip_analysis.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  1.9 KiB/  1.9 KiB]                                                \n",
      "Operation completed over 1 objects/1.9 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Set bucket name\n",
    "bucket_name = \"spbd-nyc-taxi-bucket\"\n",
    "script_name = \"trip_analysis.py\"\n",
    "\n",
    "# Upload the Python script\n",
    "!gsutil cp ./{script_name} gs://{bucket_name}/scripts/\n",
    "\n",
    "# Upload processed data\n",
    "# !gsutil -m cp -r ../../data/processed/yellow_tripdata_2024-01.parquet gs://{bucket_name}/data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on operation [projects/epita-spbd-nyc-da/regions/europe-west9/operations/9374adc8-ad50-3e7d-b717-44193fd16048].\n",
      "Waiting for cluster creation operation...                                      \n",
      "\u001b[1;33mWARNING:\u001b[0m Consider using Auto Zone rather than selecting a zone manually. See https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone\n",
      "\u001b[1;33mWARNING:\u001b[0m Failed to validate permissions required for default service account: '842263258747-compute@developer.gserviceaccount.com'. Cluster creation could still be successful if required permissions have been granted to the respective service accounts as mentioned in the document https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#dataproc_service_accounts_2. This could be due to Cloud Resource Manager API hasn't been enabled in your project '842263258747' before or it is disabled. Enable it by visiting 'https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=842263258747'.\n",
      "\u001b[1;33mWARNING:\u001b[0m The firewall rules for specified network or subnetwork would allow ingress traffic from 0.0.0.0/0, which could be a security risk.\n",
      "\u001b[1;33mWARNING:\u001b[0m The specified custom staging bucket 'dataproc-staging-europe-west9-842263258747-telg9nke' is not using uniform bucket level access IAM configuration. It is recommended to update bucket to enable the same. See https://cloud.google.com/storage/docs/uniform-bucket-level-access.\n",
      "Waiting for cluster creation operation...done.                                 \n",
      "Created [https://dataproc.googleapis.com/v1/projects/epita-spbd-nyc-da/regions/europe-west9/clusters/spbd-nyc-taxi-cluster] Cluster placed in zone [europe-west9-a].\n"
     ]
    }
   ],
   "source": [
    "cluster_name = \"spbd-nyc-taxi-cluster\"\n",
    "region = \"europe-west9\"\n",
    "machine_type=\"n2-standard-2\"\n",
    "\n",
    "!gcloud dataproc clusters create {cluster_name} \\\n",
    "    --region={region} \\\n",
    "    --zone={region}-a \\\n",
    "    --master-machine-type={machine_type} \\\n",
    "    --worker-machine-type={machine_type} \\\n",
    "    --num-workers=2 \\\n",
    "    --image-version=2.0-debian10 \\\n",
    "    --scopes=default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [0369013ca66646f38c17e3ff27281bd3] submitted.\n",
      "Waiting for job output...\n",
      "24/05/25 12:23:48 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "24/05/25 12:23:48 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "24/05/25 12:23:48 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/05/25 12:23:48 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "24/05/25 12:23:49 INFO org.sparkproject.jetty.util.log: Logging initialized @4900ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "24/05/25 12:23:49 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_412-b08\n",
      "24/05/25 12:23:49 INFO org.sparkproject.jetty.server.Server: Started @5027ms\n",
      "24/05/25 12:23:49 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@40a12214{HTTP/1.1, (http/1.1)}{0.0.0.0:39411}\n",
      "24/05/25 12:23:50 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spbd-nyc-taxi-cluster-m/10.200.0.18:8032\n",
      "24/05/25 12:23:50 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at spbd-nyc-taxi-cluster-m/10.200.0.18:10200\n",
      "24/05/25 12:23:52 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "24/05/25 12:23:52 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/05/25 12:23:53 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1716639740919_0001\n",
      "24/05/25 12:23:54 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spbd-nyc-taxi-cluster-m/10.200.0.18:8030\n",
      "24/05/25 12:23:57 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=288; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-europe-west9-842263258747-ucxdsplb/a53a30a5-03ee-4b39-9b28-10fb586f7eb4/spark-job-history\n",
      "24/05/25 12:23:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "+---------------------+------------------------+\n",
      "|average_trip_duration|average_trip_distance_km|\n",
      "+---------------------+------------------------+\n",
      "|    959.4922729603659|      3.4408639020108507|\n",
      "+---------------------+------------------------+\n",
      "\n",
      "+----+-----------+-----+------------------+------------------+\n",
      "|hour|day_of_week|month|      avg_duration|      avg_distance|\n",
      "+----+-----------+-----+------------------+------------------+\n",
      "|   0|          1|    1| 763.2528604118993|3.5783930983861065|\n",
      "|   0|          1|    2|1054.0252525252524|3.4565260078008913|\n",
      "|   0|          1|    3| 1038.832881172002| 3.585004270544865|\n",
      "|   0|          1|    4| 873.1338167435729| 3.540788315738642|\n",
      "|   0|          1|    5|1087.5949214026602| 3.868367597078292|\n",
      "|   0|          1|    6| 755.6909090909091|3.7101248748087308|\n",
      "|   1|          1|    1| 926.6111488783141| 3.589771644423448|\n",
      "|   1|          1|    2| 806.9233289646133|3.5222848058837375|\n",
      "|   1|          1|    3| 841.2996941896024|3.8156165627259786|\n",
      "|   1|          1|    4| 993.4912280701755| 3.677837338821348|\n",
      "|   1|          1|    5| 768.6707597851113|3.8548756620560365|\n",
      "|   1|          1|    6| 880.6334975369458| 4.142152136273498|\n",
      "|   2|          1|    1| 790.8755760368664|3.9479027021203015|\n",
      "|   2|          1|    2|  915.942507068803| 3.830670694667496|\n",
      "|   2|          1|    3|1071.8077557755776|4.1842317986313615|\n",
      "|   2|          1|    4| 830.5751072961374| 4.171841941953518|\n",
      "|   2|          1|    5| 916.9881936245573| 4.364224807740289|\n",
      "|   2|          1|    6| 736.4847161572052| 4.573551003984213|\n",
      "|   3|          1|    1|  847.256906077348| 4.284550771780388|\n",
      "|   3|          1|    2| 1063.718309859155| 4.104050606172266|\n",
      "+----+-----------+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+------------------+-----+\n",
      "|   pickup_latitude|  pickup_longitude|count|\n",
      "+------------------+------------------+-----+\n",
      "| 40.82100296020508|-73.95466613769531|   39|\n",
      "| 40.77378845214844|-73.87093353271484|   15|\n",
      "|40.773738861083984| -73.8708724975586|   14|\n",
      "|  40.7741813659668|-73.87300872802734|   14|\n",
      "| 40.77376174926758| -73.8708724975586|   14|\n",
      "|  40.7741584777832|-73.87303924560547|   14|\n",
      "| 40.77376937866211|-73.87093353271484|   14|\n",
      "| 40.77410888671875|-73.87303161621094|   13|\n",
      "| 40.77370071411133| -73.8708724975586|   13|\n",
      "| 40.77381134033203|-73.87095642089844|   13|\n",
      "+------------------+------------------+-----+\n",
      "\n",
      "+------------------+------------------+-----+\n",
      "|  dropoff_latitude| dropoff_longitude|count|\n",
      "+------------------+------------------+-----+\n",
      "| 40.82100296020508|-73.95466613769531|   39|\n",
      "| 40.76057815551758|-74.00276947021484|   10|\n",
      "|40.750389099121094|-73.99468231201172|   10|\n",
      "| 40.76055145263672|-74.00276947021484|    9|\n",
      "| 40.75046157836914|-73.99466705322266|    8|\n",
      "| 40.75040817260742|-73.99465942382812|    8|\n",
      "|40.750370025634766|-73.99466705322266|    8|\n",
      "| 40.75043869018555|-73.99463653564453|    7|\n",
      "|  40.7501106262207|-73.99490356445312|    7|\n",
      "| 40.76948547363281|-73.86331939697266|    7|\n",
      "+------------------+------------------+-----+\n",
      "\n",
      "24/05/25 12:24:31 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@40a12214{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
      "24/05/25 12:24:31 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=170; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-europe-west9-842263258747-ucxdsplb/a53a30a5-03ee-4b39-9b28-10fb586f7eb4/spark-job-history/application_1716639740919_0001.inprogress\n",
      "24/05/25 12:24:32 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_rename. latencyMs=207; previousMaxLatencyMs=0; operationCount=1; context=rename(gs://dataproc-temp-europe-west9-842263258747-ucxdsplb/a53a30a5-03ee-4b39-9b28-10fb586f7eb4/spark-job-history/application_1716639740919_0001.inprogress -> gs://dataproc-temp-europe-west9-842263258747-ucxdsplb/a53a30a5-03ee-4b39-9b28-10fb586f7eb4/spark-job-history/application_1716639740919_0001)\n",
      "Job [0369013ca66646f38c17e3ff27281bd3] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-europe-west9-842263258747-telg9nke/google-cloud-dataproc-metainfo/a53a30a5-03ee-4b39-9b28-10fb586f7eb4/jobs/0369013ca66646f38c17e3ff27281bd3/\n",
      "driverOutputResourceUri: gs://dataproc-staging-europe-west9-842263258747-telg9nke/google-cloud-dataproc-metainfo/a53a30a5-03ee-4b39-9b28-10fb586f7eb4/jobs/0369013ca66646f38c17e3ff27281bd3/driveroutput\n",
      "jobUuid: c6ec64bc-598d-3a98-badb-8a0bb78b7d52\n",
      "placement:\n",
      "  clusterName: spbd-nyc-taxi-cluster\n",
      "  clusterUuid: a53a30a5-03ee-4b39-9b28-10fb586f7eb4\n",
      "pysparkJob:\n",
      "  mainPythonFileUri: gs://spbd-nyc-taxi-bucket/scripts/trip_analysis.py\n",
      "reference:\n",
      "  jobId: 0369013ca66646f38c17e3ff27281bd3\n",
      "  projectId: epita-spbd-nyc-da\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2024-05-25T12:24:36.003490Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2024-05-25T12:23:41.449352Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2024-05-25T12:23:41.480414Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2024-05-25T12:23:42.079610Z'\n",
      "yarnApplications:\n",
      "- name: NYC Taxi Trip Analysis\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://spbd-nyc-taxi-cluster-m:8088/proxy/application_1716639740919_0001/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark \\\n",
    "    gs://spbd-nyc-taxi-bucket/scripts/{script_name} \\\n",
    "    --cluster={cluster_name} \\\n",
    "    --region={region}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_ID                            TYPE     STATUS\n",
      "0369013ca66646f38c17e3ff27281bd3  pyspark  DONE\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs list --cluster=spbd-nyc-taxi-cluster --region=europe-west9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on operation [projects/epita-spbd-nyc-da/regions/europe-west9/operations/9ffd6b40-182e-31ff-87a8-2617e89e2fe1].\n",
      "Waiting for cluster deletion operation...done.                                 \n",
      "Deleted [https://dataproc.googleapis.com/v1/projects/epita-spbd-nyc-da/regions/europe-west9/clusters/spbd-nyc-taxi-cluster].\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters delete spbd-nyc-taxi-cluster --region=europe-west9 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
