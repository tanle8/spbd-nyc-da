{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Analysis\n",
    "\n",
    "Tasks:\n",
    "- Evaluate average trip durations and distances.\n",
    "- Analyze these metrics by different times of day, days of the week, and months to uncover patterns.\n",
    "- Identify the top 10 pickup and drop-off locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/21 00:04:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Trip Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "TRAIN_PROCESSED = \"../../data/processed/train_processed.parquet\"\n",
    "\n",
    "df = spark.read.parquet(TRAIN_PROCESSED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Average Trip Durations and Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------------+\n",
      "|average_trip_duration|average_trip_distance_km|\n",
      "+---------------------+------------------------+\n",
      "|    959.4922729603659|       3.440863902010865|\n",
      "+---------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate average trip duration and distance\n",
    "avg_trip_stats = df.select(\n",
    "    avg(\"trip_duration\").alias(\"average_trip_duration\"),\n",
    "    avg(\"trip_distance_km\").alias(\"average_trip_distance_km\")\n",
    ")\n",
    "avg_trip_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+------------------+------------------+\n",
      "|hour|day_of_week|month|      avg_duration|      avg_distance|\n",
      "+----+-----------+-----+------------------+------------------+\n",
      "|   0|          1|    1| 886.5953488372093|3.4501344315200257|\n",
      "|   0|          1|    2| 869.2132132132132|3.4308650655189044|\n",
      "|   0|          1|    3| 910.7371512481644|3.4837120887246047|\n",
      "|   0|          1|    4|1017.5971153846153|3.5646453594082033|\n",
      "|   0|          1|    5| 885.5396618985695|3.5978548803406696|\n",
      "|   0|          1|    6| 909.3114840062926|3.5613075183708562|\n",
      "|   1|          1|    1| 763.2528604118993|3.5783930983861048|\n",
      "|   1|          1|    2|1054.0252525252524|3.4565260078008913|\n",
      "|   1|          1|    3| 1038.832881172002| 3.585004270544865|\n",
      "|   1|          1|    4| 886.9234943027673|  3.57010856369486|\n",
      "|   1|          1|    5| 1020.668815071889| 3.706051193649675|\n",
      "|   1|          1|    6| 871.3853718500308|3.6974887099880167|\n",
      "|   2|          1|    1| 926.6111488783141|3.5897716444234544|\n",
      "|   2|          1|    2| 806.9233289646133| 3.522284805883737|\n",
      "|   2|          1|    3|  947.505928853755| 3.805873918292465|\n",
      "|   2|          1|    4| 873.1338167435729|3.5407883157386406|\n",
      "|   2|          1|    5|1087.5949214026602|3.8683675970782896|\n",
      "|   2|          1|    6| 755.6909090909091| 3.710124874808732|\n",
      "|   3|          1|    1| 790.8755760368664| 3.947902702120302|\n",
      "|   3|          1|    2|  915.942507068803|3.8306706946674955|\n",
      "+----+-----------+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek, month\n",
    "\n",
    "# Add time columns (if not already added in previous feature engineering eda notebook)\n",
    "df = df.withColumn(\"hour\", hour(\"pickup_datetime\"))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(\"pickup_datetime\"))\n",
    "df = df.withColumn(\"month\", month(\"pickup_datetime\"))\n",
    "\n",
    "# Cache the df\n",
    "df.cache()\n",
    "\n",
    "# Trigger caching with an action\n",
    "df.count()\n",
    "\n",
    "# Group by new time columns and calculate averages\n",
    "time_analysis = df.groupBy(\"hour\", \"day_of_week\", \"month\").agg(\n",
    "    avg(\"trip_duration\").alias(\"avg_duration\"),\n",
    "    avg(\"trip_distance_km\").alias(\"avg_distance\")\n",
    ").orderBy(\"day_of_week\", \"hour\", \"month\")\n",
    "\n",
    "time_analysis.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Top 10 Pickup and Drop-off Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----+\n",
      "|   pickup_latitude|  pickup_longitude|count|\n",
      "+------------------+------------------+-----+\n",
      "| 40.82100296020508|-73.95466613769531|   39|\n",
      "| 40.77378845214844|-73.87093353271484|   15|\n",
      "| 40.77376937866211|-73.87093353271484|   14|\n",
      "| 40.77376174926758| -73.8708724975586|   14|\n",
      "|40.773738861083984| -73.8708724975586|   14|\n",
      "|  40.7741584777832|-73.87303924560547|   14|\n",
      "|  40.7741813659668|-73.87300872802734|   14|\n",
      "| 40.77376174926758|-73.87091064453125|   13|\n",
      "| 40.77381134033203|-73.87095642089844|   13|\n",
      "| 40.77410888671875|-73.87303161621094|   13|\n",
      "+------------------+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----+\n",
      "|  dropoff_latitude| dropoff_longitude|count|\n",
      "+------------------+------------------+-----+\n",
      "| 40.82100296020508|-73.95466613769531|   39|\n",
      "|40.750389099121094|-73.99468231201172|   10|\n",
      "| 40.76057815551758|-74.00276947021484|   10|\n",
      "| 40.76055145263672|-74.00276947021484|    9|\n",
      "|40.750370025634766|-73.99466705322266|    8|\n",
      "| 40.75040817260742|-73.99465942382812|    8|\n",
      "| 40.75046157836914|-73.99466705322266|    8|\n",
      "|40.770591735839844|-73.86512756347656|    7|\n",
      "| 40.75014877319336|-73.99126434326172|    7|\n",
      "| 40.76839828491211|-73.86177825927734|    7|\n",
      "+------------------+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Top 10 pickup locations\n",
    "top_pickup_locations = df.groupBy(\"pickup_latitude\", \"pickup_longitude\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "top_pickup_locations.show()\n",
    "\n",
    "# Top 10 dropoff locations\n",
    "top_dropoff_locations = df.groupBy(\"dropoff_latitude\", \"dropoff_longitude\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "top_dropoff_locations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, vendor_id: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: int, pickup_longitude: double, pickup_latitude: double, dropoff_longitude: double, dropoff_latitude: double, store_and_fwd_flag: string, trip_duration: int, pickup_dayofweek: int, pickup_hour: int, pickup_month: int, pickup_year: int, trip_distance_km: double, hour: int, day_of_week: int, month: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpersist the df to free up resources\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the current session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
